# -*- coding: utf-8 -*-
"""TimeSeriesAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oExniAGRF8lmsb-JY61lEiCjT0mfiB5u

## Time Series Analysis Using LSTMs
"""

from google.colab import files
import pandas as pd
import io
uploaded = files.upload()
file_name = list(uploaded.keys())[0]  # Gets the name of the uploaded file
df = pd.read_excel(io.BytesIO(uploaded[file_name]),parse_dates=['date'])
print(df.columns)

print(df.columns)
print(df.describe())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber

# Ensure your DataFrame has a date column
df['date'] = pd.to_datetime(df['date'], format='mixed')

# Extract the F10.7 column and time-based features
data = df[['date', 'F10.7']]

# Replace missing values (if any) with the median
data['F10.7'].replace(-1, data['F10.7'].median(), inplace=True)

# Add time-based features
data['day_of_year'] = data['date'].dt.dayofyear
data['month'] = data['date'].dt.month
data['day_of_week'] = data['date'].dt.dayofweek

# Drop the date column for scaling
data = data.drop(columns=['date'])

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Create sequences for training
def create_sequences(data, seq_length):
    sequences = []
    labels = []
    for i in range(len(data) - seq_length):
        sequences.append(data[i:i + seq_length])
        labels.append(data[i + seq_length, 0])  # Predicting 'F10.7'
    return np.array(sequences), np.array(labels)

# Define sequence length
seq_length = 120

# Create training sequences
X, y = create_sequences(scaled_data, seq_length)

# Split the data into training and testing sets
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, X.shape[2])))
model.add(LSTM(50))
model.add(Dense(1))

# Compile the model with Huber loss
model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(), metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()

# Plot training and validation mean absolute error
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.legend()
plt.show()

# Calculate RMSE
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RMSE: {rmse}')

# Make predictions for the next 60 days
last_sequence = scaled_data[-seq_length:, :]
predicted_values = []

for _ in range(60):
    # Reshape the last sequence to fit the model input
    input_sequence = last_sequence.reshape((1, seq_length, X.shape[2]))
    # Predict the next value
    predicted_value = model.predict(input_sequence)
    # Append the predicted value to the predicted_values list
    predicted_values.append(predicted_value[0][0])
    # Update the last_sequence with the predicted value
    last_sequence = np.append(last_sequence[1:], np.append(predicted_value, last_sequence[-1, 1:]).reshape(1, -1), axis=0)

# Inverse transform the predicted values
predicted_values = scaler.inverse_transform(np.concatenate([np.array(predicted_values).reshape(-1, 1), last_sequence[-60:, 1:]], axis=1))[:, 0]

# Print the predicted values as a numpy array
print(predicted_values.flatten())

"""###Time Series Using Gradient Regressor Model

"""

#Tuned Gradient Regressor Model

# Step 1: Upload the Excel file in Google Colab
from google.colab import files
# Step 2: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

uploaded = files.upload()
# Step 3: Load the data
file_name = list(uploaded.keys())[0]  # Get the uploaded file name
df = pd.read_excel(file_name, sheet_name='F10.7')
# Display the first few rows of the dataframe
print(df.head())
print(df.describe())
print(len(df))

# Step 4: Define the feature columns
feature_columns = ['F10.7_1', 'F10.7_2', 'F10.7_3', 'F10.7_4', 'Average of All cycles ']

# Step 5: Feature Engineering - Add lag features and rolling averages
for i in range(1, 4):
    df[f'F10.7_avg_lag_{i}'] = df['Average of All cycles '].shift(i)
df['F10.7_avg_roll_mean'] = df['Average of All cycles '].rolling(window=3).mean()

# Drop rows with NaN values after adding lag features
df = df.dropna(subset=feature_columns + [f'F10.7_avg_lag_{i}' for i in range(1, 4)] + ['F10.7_avg_roll_mean'])

# Create a target placeholder by shifting the average of all cycles
df['F10.7_5_pseudo'] = df['Average of All cycles '].shift(-1)
df = df.dropna(subset=['F10.7_5_pseudo'])

# Separate features and target
feature_columns_extended = feature_columns + [f'F10.7_avg_lag_{i}' for i in range(1, 4)] + ['F10.7_avg_roll_mean']
X = df[feature_columns_extended]
y = df['F10.7_5_pseudo']

# Step 6: Visualize the data
plt.figure(figsize=(10, 6))
for column in feature_columns:
    plt.plot(df.index, df[column], label=column)
plt.plot(df.index, df['F10.7_5_pseudo'], label='Pseudo F10.7_5', linewidth=3, linestyle='--')
plt.legend()
plt.xlabel('Time')
plt.ylabel('F10.7 Values')
plt.title('F10.7 Values Over Time')
plt.show()

# Step 7: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 8: Initialize the Gradient Boosting model with GridSearchCV for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=5,
                           n_jobs=-1,
                           scoring='neg_mean_squared_error',
                           verbose=2,
                           error_score='raise')

# Step 9: Train the model using GridSearchCV
grid_search.fit(X_train, y_train)

# Step 10: Get the best estimator
best_model = grid_search.best_estimator_

# Step 11: Cross-validation to evaluate the best model
scores = cross_val_score(best_model, X, y, cv=5, scoring='neg_mean_squared_error')
mse_cv = -scores.mean()
rmse_cv = np.sqrt(mse_cv)
print(f'Mean Squared Error (Cross-Validation): {mse_cv}')
print(f'Root Mean Squared Error (Cross-Validation): {rmse_cv}')

# Step 12: Predict the F10.7_5 values for the entire dataset
df['F10.7_5_predicted'] = best_model.predict(X)

# Step 13: Print the forecast values
print(df[['F10.7_5_pseudo', 'F10.7_5_predicted']])

# Step 14: Plot the predicted vs actual values
plt.figure(figsize=(10, 6))
plt.plot(df['F10.7_5_pseudo'], label='Pseudo Actual F10.7_5', linestyle='--')
plt.plot(df['F10.7_5_predicted'], label='Predicted F10.7_5')
plt.legend()
plt.xlabel('Time')
plt.ylabel('F10.7 Values')
plt.title('Pseudo Actual vs Predicted F10.7_5 Values')
plt.show()

import numpy as np
import pandas as pd
from datetime import datetime

# Assuming df is your DataFrame and it has columns 'F10.7_5_pseudo' and 'F10.7_5_predicted'
print(df[['F10.7_5_pseudo', 'F10.7_5_predicted']])

# Start date of the solar cycle
start_date = datetime(2019, 8, 1)

# Today's date
today = datetime.now()

# Calculate the difference in days
days_diff = (today - start_date).days

# Find the corresponding index
start_index = days_diff

# Ensure the DataFrame has enough data
if start_index + 60 <= len(df):
    # Extract the next 60 days' values from the 'F10.7_5_predicted' column
    next_60_days_values = df.iloc[start_index:start_index + 60]['F10.7_5_predicted'].values

    # Store in a numpy array
    gradregressorpred = np.array(next_60_days_values)

"""##Time Series Analysis using Facebook Prophet

"""

import pandas as pd
from prophet import Prophet
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt


cdf=df
columns_to_drop=["SN","Ap"]
cdf.drop(columns=columns_to_drop,inplace=True)
new_column_names = {'date': 'ds', 'F10.7': 'y'}
cdf.rename(columns=new_column_names, inplace=True)


# Define the function to determine solar cycle phase
def determine_solar_cycle_phase(ds):
    date = pd.to_datetime(ds)
    str_cycle_starts = ["02-1944", "04-1954", "10-1964", "03-1976", "09-1986", "08-1996", "12-2008", "12-2019"]
    cycle_starts = [datetime.strptime(date_str, "%m-%Y") for date_str in str_cycle_starts]
    cycle_starts = [d for d in cycle_starts if d <= date]
    if not cycle_starts:
        raise ValueError("Date is before the first recorded solar cycle start.")
    last_cycle_start = max(cycle_starts)
    years_since_cycle_start = (date.year - last_cycle_start.year) + (date.month - last_cycle_start.month) / 12

    if years_since_cycle_start < 1.5 or years_since_cycle_start > 6.5:
        return 'minimum'
    else:
        return 'maximum'

# Load your data into a DataFrame 'cdf' with columns 'ds' (date) and 'y' (value)
# Make sure to uncomment and update this line with your data source
# cdf = pd.read_csv('your_data.csv')

# Apply the solar cycle phase determination to the DataFrame
cdf['solar_maximum'] = cdf['ds'].apply(determine_solar_cycle_phase) == 'maximum'
cdf['solar_minimum'] = cdf['ds'].apply(determine_solar_cycle_phase) == 'minimum'

# Initialize the Prophet model without weekly seasonality
m = Prophet(weekly_seasonality=False)

# Add custom seasonality for solar maximum and minimum phases
m.add_seasonality(name='solar_maximum_period', period=365.25*11, fourier_order=8, condition_name='solar_maximum')
m.add_seasonality(name='solar_minimum_period', period=365.25*11, fourier_order=3, condition_name='solar_minimum')

# Fit the model
m.fit(cdf)

# Create future dataframe
future = m.make_future_dataframe(periods=365)

# Apply the solar cycle phase determination to the future DataFrame
future['solar_maximum'] = future['ds'].apply(determine_solar_cycle_phase) == 'maximum'
future['solar_minimum'] = future['ds'].apply(determine_solar_cycle_phase) == 'minimum'

# Make predictions
forecast = m.predict(future)
ft = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(365)

# Plot the forecast
plt.figure(figsize=(12, 8))
plt.plot(cdf['ds'], cdf['y'], label='Observed')
plt.plot(forecast['ds'], forecast['yhat'], label='Forecast', color='orange')
plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='orange', alpha=0.2)
plt.xlabel('Date')
plt.ylabel('Value')
plt.title('Forecast with Prophet')
plt.legend()
plt.show()

# Get the top 10 rows of the 'yhat' column
top_10_yhat = ft['yhat'].head(30)

# Convert to a NumPy array
prophet_predictions = np.round(np.array(top_10_yhat), 1)

# Print the top 10 predictions
print("Top 10 Prophet Predictions:\n", prophet_predictions)

"""## Ensemble Model"""

import numpy as np
import pandas as pd

# Assign weights based on model performance (higher weight for better performance)
weights = {
    'lstm': 0.2,  # Weight for LSTM
    'gr': 0.7,  # Weight for Gradient Regressor
    'prophet':0.1
}

# Flatten LSTM predictions if needed
lstm_predictions = predicted_values.flatten()

# Compute weighted average ensemble predictions
ensemble_predictions = (
    weights['lstm'] * np.array(lstm_predictions) +
    weights['gr'] * np.array(gradregressorpred)+
    weights['prophet']*np.array(prophet_predictions)
)

# Print the ensemble predictions
print("Ensemble Predictions:", ensemble_predictions)